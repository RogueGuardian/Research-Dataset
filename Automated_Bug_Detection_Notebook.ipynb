{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0a4be129",
      "metadata": {
        "id": "0a4be129"
      },
      "source": [
        "# CSCE 491 - Research Code\n",
        "\n",
        "## Title: \"Automated Bug Detecting In Source Code Using Machine Learning\"\n",
        "\n",
        "## Description\n",
        "This project is to determine the viability and present a proof of concept of how machine learning can be used to detect bugs in code.\n",
        "\n",
        "---\n",
        "\n",
        "## General Notes\n",
        "- DATASET : NIST Juliet C/C++ v1.3\n",
        "    - Filters: No cpp, No Win32, No multi-file code\n",
        "- ML Libraries : Sklearn, Pytorch\n",
        "\n",
        "---\n",
        "\n",
        "## TYPE Classification\n",
        "- TYPE_BUFFER_BUGS\n",
        "    - Only buffer overflow, underflow, underread, etc\n",
        "- TYPE_NUMERICAL_BUGS\n",
        "    - Integer overflow, integer underflow, signed VS unsigned, divide by zero, etc\n",
        "- TYPE_LOGIC_BUGS\n",
        "    - Infinite Loop, Assign VS Compare, Recursion, Unchecked loop condition, etc\n",
        "- TYPE_MEMORY_BUGS\n",
        "    - NULLPTR deref, Double free, use after free, Uncontrolled memory alloc, etc\n",
        "\n",
        "---\n",
        "\n",
        "## Personal Notes\n",
        "1. Precision = Correct Classification / All Classified (In a category) (TP / (TP + FP))\n",
        "1. Recall = Correct Classification / All True (In a category) (TP / (TP + FN))\n",
        "1. F-1 Score = Balanced Mean of Precision & Recall (Both need to be good for high F-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7e6e035",
      "metadata": {
        "id": "e7e6e035"
      },
      "outputs": [],
      "source": [
        "### IMPORTS ###\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "import os\n",
        "import subprocess\n",
        "import pickle\n",
        "import scipy\n",
        "import torchtext\n",
        "import h5py\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.cuda as cuda\n",
        "\n",
        "# Misc Items\n",
        "from random import seed\n",
        "from random import shuffle\n",
        "from random import randint\n",
        "from time import time\n",
        "from math import sqrt\n",
        "\n",
        "# More Matplot\n",
        "from matplotlib import colors\n",
        "\n",
        "# Sklearn Items\n",
        "from sklearn.model_selection import *\n",
        "from sklearn.metrics import *\n",
        "from sklearn.ensemble import *\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Pytorch and Parsing stuff\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pprint import pprint\n",
        "\n",
        "# JUST USING CLANG\n",
        "import clang.cindex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "495fb7a0",
      "metadata": {
        "id": "495fb7a0"
      },
      "outputs": [],
      "source": [
        "## Needed for the random shuffle later\n",
        "seed(int(time()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### SELECTION CONSTANTS ###\n",
        "## These constants select how the rest of the code will run and work\n",
        "\n",
        "## TOKEN TYPE\n",
        "# Selection variable for which type of tokens to use [0 for TOKEN | 2 for CLANG Tokens | 3 for EXTENDED Tokens]\n",
        "USING_TOKEN_TYPE = 2\n",
        "\n",
        "## MULTICLASS\n",
        "# Selection variable to select multiclass [0] or binary [1] training\n",
        "USING_CLASSES = 0"
      ],
      "metadata": {
        "id": "f7gIyc3_UvLw"
      },
      "id": "f7gIyc3_UvLw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4131318",
      "metadata": {
        "id": "f4131318"
      },
      "outputs": [],
      "source": [
        "### CONSTANTS ###\n",
        "\n",
        "## Only use when needed to have a \"constant\" random seed\n",
        "# RANDOM_SEED = 69\n",
        "\n",
        "# Use GPU over CPU\n",
        "DEVICE = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(DEVICE)\n",
        "\n",
        "### Relevant for RFC\n",
        "# Number of NGRAMS (universal for program)\n",
        "NGRAMS = 4\n",
        "# Number of estimators for RFC\n",
        "ESTIMATORS = 64\n",
        "\n",
        "# Ratio of Training set to be used as Testing\n",
        "## NOTE - I AM NOT SURE THIS IS USED ANYMORE\n",
        "TESTTRAINRATIO = 0.1\n",
        "\n",
        "### Relevant for the CLANG parsing [DO NOT TOUCH]\n",
        "INDENT = 4\n",
        "K = clang.cindex.CursorKind\n",
        "###\n",
        "\n",
        "# Conversion Dictionary to turn tokens to integers (ease of use in ML)\n",
        "TOKENLIST = [\n",
        "    \"KEYWORD\",\n",
        "    \"PUNCTUATION\",\n",
        "    \"IDENTIFIER\",\n",
        "    \"LITERAL\",\n",
        "    \"COMMENT\"\n",
        "]\n",
        "TOKENDICT = {val: idx for idx,val in enumerate(TOKENLIST)}\n",
        "INVTOKENDICT = {idx: val for idx,val in enumerate(TOKENLIST)}\n",
        "\n",
        "# Extended Token Conversions\n",
        "EXTENDEDLIST = [\n",
        "    \"KEYWORD\",\n",
        "    \"PUNCTUATION\",\n",
        "    \"IDENTIFIER\",\n",
        "    \"LITERAL\",\n",
        "    \"COMMENT\",\n",
        "    \"LOOPFOR\",\n",
        "    \"LOOPWHILE\",\n",
        "    \"CMPEQUALS\",\n",
        "    \"CMPNEQUALS\",\n",
        "    \"CMPLT\",\n",
        "    \"CMPGT\",\n",
        "    \"CMPLTE\",\n",
        "    \"CMPGTE\",\n",
        "    \"IFSTMT\",\n",
        "    \"ELSESTMT\",\n",
        "    \"TYPEINT\",\n",
        "    \"TYPECHAR\",\n",
        "    \"TYPEVOID\",\n",
        "    \"TYPEDOUBLE\",\n",
        "    \"TYPEFLOAT\"\n",
        "]\n",
        "EXTENDEDTOKENS = {val: idx for idx,val in enumerate(EXTENDEDLIST)}\n",
        "INVEXTENDEDTOKENS = {idx: val for idx,val in enumerate(EXTENDEDLIST)}\n",
        "\n",
        "clangTokenList = [\n",
        "    \"KEYWORD\",\n",
        "    \"PUNCTUATION\",\n",
        "    \"IDENTIFIER\",\n",
        "    \"LITERAL\",\n",
        "    \"COMMENT\",\n",
        "    \"LOOPFOR\",\n",
        "    \"LOOPWHILE\",\n",
        "    \"CMPEQUALS\",\n",
        "    \"CMPNEQUALS\",\n",
        "    \"CMPLT\",\n",
        "    \"CMPGT\",\n",
        "    \"CMPLTE\",\n",
        "    \"CMPGTE\",\n",
        "    \"IFSTMT\",\n",
        "    \"ELSESTMT\",\n",
        "    \"TYPEINT\",\n",
        "    \"TYPECHAR\",\n",
        "    \"TYPEVOID\",\n",
        "    \"TYPEDOUBLE\",\n",
        "    \"TYPEFLOAT\",\n",
        "    \"PARENL\",\n",
        "    \"PARENR\",\n",
        "    \"BRACESL\",\n",
        "    \"BRACESR\",\n",
        "    \"ASSIGN\",\n",
        "    \"PTR\",\n",
        "    \"REF\"\n",
        "]\n",
        "\n",
        "## Relevant for removing the unecessary (I.E. C++ specific) token types\n",
        "## NOTE - The \"__class__\" MUST stay\n",
        "## NOTE - The \"CXX\" should be removed if the code is extended to clases\n",
        "## NOTE - The \"OMP\" is only used for multithreadding (I DO NOT THINK IT WILL BE USEFUL)\n",
        "for x in dir(clang.cindex.CursorKind):\n",
        "    if(x == \"__class__\"):\n",
        "        break\n",
        "    if(x[:3] == \"CXX\" or x[:5] == \"CLASS\"):\n",
        "        continue\n",
        "    # TODO : MAYBE THESE ARE NECESSARY TOKENS IDK (MIGHT REMOVE LATER)\n",
        "    if(x[:3] == \"OMP\"):\n",
        "        continue\n",
        "    clangTokenList.append(x)\n",
        "\n",
        "CLANGTOKENS = dict()\n",
        "for i,x in enumerate(clangTokenList):\n",
        "    CLANGTOKENS[x] = i\n",
        "\n",
        "\n",
        "### CLASS DICTIONARIES\n",
        "CLASSDICT = []\n",
        "if(USING_CLASSES == 0):\n",
        "    CLASSDICT = {0:\"BUFFER\", 1:\"CLEAN\", 2:\"LOGIC\", 3:\"MEMORY\", 4:\"NUMERICAL\"}\n",
        "else:\n",
        "    CLASSDICT = {0:\"0\", 1:\"1\"}\n",
        "\n",
        "N_CLASSES = len(CLASSDICT)\n",
        "\n",
        "# JUST FOR VERIFICATION (5, 20, 176)\n",
        "print(len(TOKENDICT), len(EXTENDEDTOKENS), len(CLANGTOKENS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "646d8696",
      "metadata": {
        "id": "646d8696"
      },
      "outputs": [],
      "source": [
        "### CLANG FUNCTIONS ###     \n",
        "# Create tokens of the form \"(TOKEN_TYPE_INT, TOKEN_STRING)\"\n",
        "def tokenizeString(cString):\n",
        "    retList = []\n",
        "    idx = clang.cindex.Index.create()\n",
        "    tu = idx.parse('tmp.c', args=[''], unsaved_files=[('tmp.c', cString)], options=0)\n",
        "    for t in tu.get_tokens(extent=tu.cursor.extent):\n",
        "        retList.append((TOKENDICT[str(t.kind.name)], t.spelling))\n",
        "    return retList\n",
        "\n",
        "# Create tokens of the form \"(TOKEN_TYPE_STR)\"\n",
        "def tokenizeWithoutValue(cString):\n",
        "    retList = []\n",
        "    idx = clang.cindex.Index.create()\n",
        "    tu = idx.parse('tmp.c', args=[''], unsaved_files=[('tmp.c', cString)], options=0)\n",
        "    for t in tu.get_tokens(extent=tu.cursor.extent):\n",
        "        retList.append(str(t.kind.name))\n",
        "    return retList\n",
        "\n",
        "# Create tokens of the form \"(TOKEN_TYPE_STR)\" but more specified about the kinds\n",
        "# NOTE - This is a manual process and could be changed for more or less in the future\n",
        "def tokenizeExtended(cString):\n",
        "    retList = []\n",
        "    idx = clang.cindex.Index.create()\n",
        "    tu = idx.parse('tmp.c', args=[''], unsaved_files=[('tmp.c', cString)], options=0)\n",
        "    for t in tu.get_tokens(extent=tu.cursor.extent):\n",
        "        tok = str(t.kind.name)\n",
        "        \n",
        "        if(t.spelling == \"for\"):\n",
        "            tok = \"LOOPFOR\"\n",
        "        elif(t.spelling == \"while\"):\n",
        "            tok = \"LOOPWHILE\"        \n",
        "        elif(t.spelling == \"==\"):\n",
        "            tok = \"CMPEQUALS\" \n",
        "        elif(t.spelling == \"!=\"):\n",
        "            tok = \"CMPNEQUALS\"     \n",
        "        elif(t.spelling == \"<\"):\n",
        "            tok = \"CMPLT\"       \n",
        "        elif(t.spelling == \">\"):\n",
        "            tok = \"CMPGT\"        \n",
        "        elif(t.spelling == \"<=\"):\n",
        "            tok = \"CMPLTE\"      \n",
        "        elif(t.spelling == \">=\"):\n",
        "            tok = \"CMPGTE\" \n",
        "        elif(t.spelling == \"if\"):\n",
        "            tok = \"IFSTMT\"\n",
        "        elif(t.spelling == \"else\"):\n",
        "            tok = \"ELSESTMT\"     \n",
        "        elif(t.spelling == \"int\"):\n",
        "            tok = \"TYPEINT\"  \n",
        "        elif(t.spelling == \"char\"):\n",
        "            tok = \"TYPECHAR\"\n",
        "        elif(t.spelling == \"void\"):\n",
        "            tok = \"TYPEVOID\"\n",
        "        elif(t.spelling == \"double\"):\n",
        "            tok = \"TYPEDOUBLE\"\n",
        "        elif(t.spelling == \"float\"):\n",
        "            tok = \"TYPEFLOAT\"\n",
        "            \n",
        "        retList.append(tok)\n",
        "    return retList\n",
        "\n",
        "# DO NOT TOUCH\n",
        "def is_std_ns(node):\n",
        "    return node.kind == K.NAMESPACE and node.spelling == 'std'\n",
        "\n",
        "# DO NOT TOUCH\n",
        "def vit(node: clang.cindex.Cursor, indent: int, saw):\n",
        "    pre = ' ' * indent\n",
        "    k = node.kind  # type: clang.cindex.CursorKind\n",
        "    # skip printting UNEXPOSED_*\n",
        "    if not k.is_unexposed():\n",
        "        print(pre, end='')\n",
        "        print(k.name, end=' ')\n",
        "        if node.spelling:\n",
        "            print('s:', node.spelling, end=' ')\n",
        "            if node.type.spelling:\n",
        "                print('t:', node.type.spelling, end=' ')\n",
        "            # FIXME: print opcode or literal\n",
        "        print()\n",
        "    saw.add(node.hash)\n",
        "    if node.referenced is not None and node.referenced.hash not in saw:\n",
        "        vit(node.referenced, indent + INDENT, saw)\n",
        "    # FIXME: skip auto generated decls\n",
        "    skip = len([c for c in node.get_children()\n",
        "                if indent == 0 and is_std_ns(c)])\n",
        "    for c in node.get_children():\n",
        "        if not skip:\n",
        "            vit(c, indent + INDENT, saw)\n",
        "        if indent == 0 and is_std_ns(c):\n",
        "            skip -= 1\n",
        "    saw.remove(node.hash)\n",
        "    \n",
        "# NOTE - This gets the ORIGINAL form of the clang AST (I am using a modified version for results)\n",
        "# NOTE - This is not used anywhere but should remain for legacy purposes\n",
        "def clangOldAST(filename):\n",
        "    index = clang.cindex.Index.create()\n",
        "    tu = index.parse(filename)\n",
        "    vit(tu.cursor, 0, set())\n",
        "\n",
        "# Combine Nodes & Tokens from AST into linear sequence (similar to ASM in a way)\n",
        "def parseNode(node):\n",
        "    retStr = str(node.kind.name) +\"\\n\"\n",
        "    \n",
        "    numChilds = len([0 for _ in node.get_children()])\n",
        "    multiList = [\"TRANSLATION_UNIT\", \"FUNCTION_DECL\", \"COMPOUND_STMT\"]\n",
        "    if(numChilds < 2 and str(node.kind.name) not in multiList):\n",
        "        for t in node.get_tokens():\n",
        "            tok = str(t.kind.name)\n",
        "        \n",
        "            if(t.spelling == \"for\"):\n",
        "                tok = \"LOOPFOR\"\n",
        "            elif(t.spelling == \"while\"):\n",
        "                tok = \"LOOPWHILE\"\n",
        "            elif(t.spelling == \"==\"):\n",
        "                tok = \"CMPEQUALS\"\n",
        "            elif(t.spelling == \"!=\"):\n",
        "                tok = \"CMPNEQUALS\"\n",
        "            elif(t.spelling == \"<\"):\n",
        "                tok = \"CMPLT\"\n",
        "            elif(t.spelling == \">\"):\n",
        "                tok = \"CMPGT\"\n",
        "            elif(t.spelling == \"<=\"):\n",
        "                tok = \"CMPLTE\"\n",
        "            elif(t.spelling == \">=\"):\n",
        "                tok = \"CMPGTE\"\n",
        "            elif(t.spelling == \"if\"):\n",
        "                tok = \"IFSTMT\"\n",
        "            elif(t.spelling == \"else\"):\n",
        "                tok = \"ELSESTMT\"\n",
        "            elif(t.spelling == \"int\"):\n",
        "                tok = \"TYPEINT\"\n",
        "            elif(t.spelling == \"char\"):\n",
        "                tok = \"TYPECHAR\"\n",
        "            elif(t.spelling == \"void\"):\n",
        "                tok = \"TYPEVOID\"\n",
        "            elif(t.spelling == \"double\"):\n",
        "                tok = \"TYPEDOUBLE\"\n",
        "            elif(t.spelling == \"float\"):\n",
        "                tok = \"TYPEFLOAT\"\n",
        "            elif(t.spelling == \"(\"):\n",
        "                tok = \"PARENL\"\n",
        "            elif(t.spelling == \")\"):\n",
        "                tok = \"PARENR\"\n",
        "            elif(t.spelling == \"{\"):\n",
        "                tok = \"BRACESL\"\n",
        "            elif(t.spelling == \"}\"):\n",
        "                tok = \"BRACESR\"\n",
        "            elif(t.spelling == \"=\"):\n",
        "                tok = \"ASSIGN\"\n",
        "            elif(t.spelling == \"*\"):\n",
        "                tok = \"PTR\"\n",
        "            elif(t.spelling == \"&\"):\n",
        "                tok = \"REF\"\n",
        "                \n",
        "            retStr += str(tok) + \"\\n\"\n",
        "    \n",
        "    for ch in node.get_children():\n",
        "        retStr += parseNode(ch)\n",
        "        \n",
        "    return retStr\n",
        " \n",
        "# Call this to get the string form of the \"new\" CLANG AST\n",
        "def clangFileAST(filename):\n",
        "    index = clang.cindex.Index.create()\n",
        "    tu = index.parse(filename)\n",
        "    return parseNode(tu.cursor)\n",
        "\n",
        "# Returns a list of the tokens in the \"new\" CLANG AST\n",
        "def clangFileASTList(filename):\n",
        "    return clangFileAST(filename).split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "930c6bdc",
      "metadata": {
        "id": "930c6bdc"
      },
      "outputs": [],
      "source": [
        "### PICKLE FUNCTIONS ###\n",
        "# Filename constants - Makes life easier\n",
        "pickleFileFunctions = \"./Functions.pkl\"\n",
        "pickleFilePairTokens = \"./PairTokens.pkl\"\n",
        "pickleFileSingleTokens = \"./SingleTokens.pkl\"\n",
        "pickleFileClangTokens = \"./ClangTokens.pkl\"\n",
        "pickleFileUnknownFiles = \"./UnknownFiles.pkl\"\n",
        "pickleFileExtendedTokens = \"./ExtendedTokens.pkl\"\n",
        "pickleFileHPFiles = \"./HPFiles.pkl\"\n",
        "pickleFileCustomDataset = \"./CustomDataset.pkl\"\n",
        "\n",
        "def pickleIntoFile(filename, data):\n",
        "    with open(filename, \"wb\") as wFile:\n",
        "        pickle.dump(data, wFile)\n",
        "\n",
        "def pickleOutFromFile(filename):\n",
        "    with open(filename, \"rb\") as rFile:\n",
        "        return pickle.load(rFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1764487",
      "metadata": {
        "id": "e1764487"
      },
      "outputs": [],
      "source": [
        "### PARSING FUNCTIONS ###\n",
        "\n",
        "# NOTE - This is the most cursed code section\n",
        "# It's a mixture of file reading, compiling, preprocessing, and pain\n",
        "# For now this works - If you want to re-do it feel free\n",
        "\n",
        "def reverseBraces(inputStr):\n",
        "    leftcount = 0\n",
        "    rightcount = 0\n",
        "    sentinel = 0\n",
        "    newarr = []\n",
        "    \n",
        "    for line in inputStr[::-1]:\n",
        "        newarr.append(line)\n",
        "        if(sentinel == 1):\n",
        "            break\n",
        "\n",
        "        for c in line:\n",
        "            if(c == \"{\"):\n",
        "                leftcount += 1\n",
        "            elif(c == \"}\"):\n",
        "                rightcount += 1\n",
        "\n",
        "        if(leftcount == rightcount and leftcount > 0):\n",
        "            sentinel = 1\n",
        "    \n",
        "    return \"\\n\".join(newarr[::-1])\n",
        "\n",
        "def reverseUntilHashtag(inputStr, fileName):\n",
        "    newarr = []\n",
        "    for line in inputStr[::-1]:\n",
        "        if(len(line) == 0):\n",
        "            continue\n",
        "        if(line[0] == \"#\"):\n",
        "            if(fileName in line):\n",
        "                continue\n",
        "            else:\n",
        "                break\n",
        "        newarr.append(line)\n",
        "        \n",
        "    return \"\\n\".join(newarr[::-1])\n",
        "\n",
        "def parseUnknownFiles():\n",
        "    functionList = []\n",
        "    \n",
        "    # Read in files from folder given filters and parse good & bad functions / sections\n",
        "    ### THIS IS JUST FOR TESTING, FOR ACTUAL RUNS PUT MAX_COUNT AT LIKE 1000000\n",
        "    counter = 0\n",
        "    MAX_COUNT = 10000\n",
        "\n",
        "    for root, dirs, files in os.walk(\"./TestCode\"):\n",
        "        if(counter >= MAX_COUNT):\n",
        "            break\n",
        "\n",
        "        for filename in files:            \n",
        "            counter += 1\n",
        "            rFileName = root + \"/\" + filename\n",
        "                \n",
        "            ###\n",
        "        \n",
        "            tempList = []\n",
        "\n",
        "            p1 = subprocess.run(\n",
        "                [\"./unifdef\", rFileName], \n",
        "                capture_output=True,\n",
        "                encoding=\"utf-8\")\n",
        "            stage1 = p1.stdout\n",
        "            print(stage1)\n",
        "            with open(\"./tempFile.c\", \"w\") as writeTemp:\n",
        "                writeTemp.write(stage1)\n",
        "                \n",
        "            p2 = subprocess.run(\n",
        "                [\"gcc\", \"-E\", \"./tempFile.c\"], \n",
        "                capture_output=True,\n",
        "                encoding=\"utf-8\")\n",
        "            stage2 = p2.stdout\n",
        "            \n",
        "            tempList.append(reverseUntilHashtag(stage2.split(\"\\n\"), \"./tempFile.c\"))\n",
        "            typeOfCode = filename.split(\".\")[0]\n",
        "            typeOfCode = typeOfCode.split(\"-\")[-1]\n",
        "            functionList.append((tempList[0], typeOfCode, filename))\n",
        "    \n",
        "    return functionList\n",
        "\n",
        "def parseJulietFiles(USENUMS):\n",
        "    functionList = []\n",
        "    \n",
        "    # Read in files from folder given filters and parse good & bad functions / sections\n",
        "    ### THIS IS JUST FOR TESTING, FOR ACTUAL RUNS PUT MAX_COUNT AT LIKE 1000000\n",
        "    counter = 0\n",
        "    MAX_COUNT = 1000000\n",
        "\n",
        "    for root, dirs, files in os.walk(\"./Data/Juliet-C/testcases\"):\n",
        "        # BREAK CASE\n",
        "        if(counter >= MAX_COUNT):\n",
        "            break\n",
        "\n",
        "        for filename in files:\n",
        "            # FILTERS\n",
        "            if(\"TYPE_\" not in root):\n",
        "                continue\n",
        "\n",
        "            if(filename.split(\".\")[-1] != \"c\"):\n",
        "                continue\n",
        "            if(filename.split(\".\")[0][-1] in \"abcdefghijklmnopqrstuvwxyz\"):\n",
        "                continue\n",
        "            if(\"w32\" in filename):\n",
        "                continue\n",
        "            \n",
        "            # PASSING CASE\n",
        "            counter += 1\n",
        "            rFileName = root + \"/\" + filename\n",
        "            \n",
        "            number = \"1\"\n",
        "            if(USENUMS == True):\n",
        "                u = root.split(\"/\")[-1]\n",
        "                number = u.split(\"_\")[1]\n",
        "                    \n",
        "            ###\n",
        "\n",
        "            p1 = subprocess.run(\n",
        "                [\"Data/unifdef-2.12/unifdef\", \"-D\", \"OMITGOOD\", \"-U\", \"INCLUDEMAIN\", rFileName], \n",
        "                capture_output=True,\n",
        "                encoding=\"utf-8\")\n",
        "            stage1 = p1.stdout\n",
        "            with open(\"Data/tempFile.c\", \"w\") as writeTemp:\n",
        "                writeTemp.write(stage1)\n",
        "\n",
        "            p2 = subprocess.run(\n",
        "                [\"gcc\", \"-E\", \"Data/tempFile.c\"], \n",
        "                capture_output=True,\n",
        "                encoding=\"utf-8\")\n",
        "            stage2 = p2.stdout\n",
        "\n",
        "            badFunctions = reverseUntilHashtag(stage2.split(\"\\n\"), \"Data/tempFile.c\")\n",
        "            \n",
        "            ##### ADDED CODE - A\n",
        "            \n",
        "            # Split into list of functions and \"top stuff\"\n",
        "            badFunctionList = []\n",
        "            while(len(badFunctions) > 0):\n",
        "                badCode = reverseBraces(badFunctions.split(\"\\n\"))\n",
        "                badFunctionList.append(badCode)\n",
        "                badFunctions = badFunctions[:len(badFunctions) - len(badCode)]\n",
        "            \n",
        "            newBadFunctionList = []\n",
        "            for x in badFunctionList:\n",
        "                if(\"CWE\" in x):\n",
        "                    newBadFunctionList.append(x)\n",
        "                    \n",
        "            ##### ADDED CODE - A\n",
        "            \n",
        "            ##### REMOVED CODE - B\n",
        "#             functionList.append((badFunctions, number))\n",
        "            ##### REMOVED CODE - B\n",
        "    \n",
        "            functionList += [(x, number) for x in newBadFunctionList]\n",
        "\n",
        "            ###\n",
        "\n",
        "            p3 = subprocess.run(\n",
        "                [\"Data/unifdef-2.12/unifdef\", \"-D\", \"OMITBAD\", \"-U\", \"INCLUDEMAIN\", rFileName], \n",
        "                capture_output=True,\n",
        "                encoding=\"utf-8\")\n",
        "            stage3 = p3.stdout\n",
        "            with open(\"Data/tempFile.c\", \"w\") as writeTemp:\n",
        "                writeTemp.write(stage3)\n",
        "\n",
        "            p4 = subprocess.run(\n",
        "                [\"gcc\", \"-E\", \"Data/tempFile.c\"], \n",
        "                capture_output=True,\n",
        "                encoding=\"utf-8\")\n",
        "            stage4 = p4.stdout\n",
        "            \n",
        "            goodFunctions = reverseUntilHashtag(stage4.split(\"\\n\"), \"Data/tempFile.c\")\n",
        "            \n",
        "            # Extra 2 lines to remove \"good handler-function\"\n",
        "            bottomGood = reverseBraces(goodFunctions.split(\"\\n\"))\n",
        "            goodFunctions = goodFunctions[:len(goodFunctions) - len(bottomGood) - 1]\n",
        "            \n",
        "            # Split into list of functions and \"top stuff\"\n",
        "            goodFunctionList = []\n",
        "            while(len(goodFunctions) > 0):\n",
        "                goodCode = reverseBraces(goodFunctions.split(\"\\n\"))\n",
        "                goodFunctionList.append(goodCode)\n",
        "                goodFunctions = goodFunctions[:len(goodFunctions) - len(goodCode)]\n",
        "            \n",
        "            # Now a list where *PERHAPS* the last N are general and need to be at the front\n",
        "            newGoodFunctionList = []\n",
        "            goodFunctionList = goodFunctionList[::-1]\n",
        "            \n",
        "            for x in goodFunctionList:\n",
        "                if(\"static void\" in x):\n",
        "                    newGoodFunctionList.append(x)\n",
        "                \n",
        "                ##### REMOVED CODE - C\n",
        "#             if(len(goodFunctionList) != 0):\n",
        "#                 if(\"static void\" not in goodFunctionList[0]):\n",
        "#                     for x in range(len(goodFunctionList)-1):\n",
        "#                         newGoodFunctionList.append(goodFunctionList[0] + goodFunctionList[x+1])\n",
        "#                 else:\n",
        "#                     newGoodFunctionList = goodFunctionList    \n",
        "#             else:\n",
        "#                 newGoodFunctionList = goodFunctionList\n",
        "                ##### REMOVED CODE - C\n",
        "            \n",
        "            # Add Tuples of Strings of Function[s] to list\n",
        "            functionList += [(x, \"CLEAN\") for x in newGoodFunctionList]\n",
        "            \n",
        "    # Return\n",
        "    return functionList\n",
        "\n",
        "# Function to remove function name and replace with generic \"function\" (focus more on code not fn name)\n",
        "def removeFuncName(reg, replace, inputStr):\n",
        "    newStr = []\n",
        "    for l in inputStr.split(\"\\n\"):\n",
        "        newStr.append(re.sub(reg, replace, l))\n",
        "    return \"\\n\".join(newStr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PICKLE Section\n",
        "\n",
        "- This just runs the above commands for reading, parsing, and tokenizing the files but pickles them at the end to save time\n",
        "- It checks for those files in the current directory and if they exist, then it just loads them into memory\n",
        "    - This saves a lot of time for re-running the program and not re-reading all the files every time"
      ],
      "metadata": {
        "id": "1jv0GztEXSie"
      },
      "id": "1jv0GztEXSie"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61907dcc",
      "metadata": {
        "id": "61907dcc"
      },
      "outputs": [],
      "source": [
        "functionCore = []\n",
        "\n",
        "# Preproc and Pickle\n",
        "if(os.path.isfile(pickleFileFunctions)):\n",
        "    ## Already Pickled so just load in\n",
        "    functionCore = pickleOutFromFile(pickleFileFunctions)\n",
        "else:\n",
        "    # Parse the files\n",
        "    functionPairs = parseJulietFiles(True)\n",
        "    \n",
        "    # Some preprocessing and minor validation\n",
        "    functionPreproc = [x for x in functionPairs if x[0] != \"\"]\n",
        "    \n",
        "    ## Remove function names\n",
        "    functionCore = [(removeFuncName('(good|CWE)(.*)\\(', 'function(', x[0]), x[1]) for x in functionPreproc]\n",
        "\n",
        "    ## Pickling into file the lists of functions as single strings\n",
        "    pickleIntoFile(pickleFileFunctions, functionCore)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unknownFiles = []\n",
        "\n",
        "# Preproc and Pickle\n",
        "if(os.path.isfile(pickleFileUnknownFiles)):\n",
        "    ## Already Pickled so just load in\n",
        "    unknownFiles = pickleOutFromFile(pickleFileUnknownFiles)\n",
        "else:\n",
        "    # Parse the files\n",
        "    functionPairs = parseUnknownFiles()\n",
        "    \n",
        "    # Some preprocessing and minor validation\n",
        "    functionPreproc = [x for x in functionPairs if x[0] != \"\"]\n",
        "    \n",
        "    ## Remove function names\n",
        "    unknownFiles = [(removeFuncName('(good|CWE)(.*)\\(', 'function(', x[0]), x[1]) for x in functionPreproc]\n",
        "\n",
        "    ## Pickling into file the lists of functions as single strings\n",
        "    pickleIntoFile(pickleFileUnknownFiles, unknownFiles)"
      ],
      "metadata": {
        "id": "aWVtXVj8XuX-"
      },
      "id": "aWVtXVj8XuX-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32fa41f3",
      "metadata": {
        "id": "32fa41f3"
      },
      "outputs": [],
      "source": [
        "tokenizedFuncs = []\n",
        "\n",
        "if(USING_TOKEN_TYPE == 0):\n",
        "    # Tokenize and Pickle\n",
        "    if(os.path.isfile(pickleFileSingleTokens)):\n",
        "        ## Already Tokenized so load\n",
        "        tokenizedFuncs = pickleOutFromFile(pickleFileSingleTokens)\n",
        "    else:\n",
        "        ## Tokenizing Functions\n",
        "        tokenizedFuncs = [(tokenizeWithoutValue(x[0]), x[1]) for x in functionCore]\n",
        "        ## Pickling into file the lists of functions as single strings\n",
        "        pickleIntoFile(pickleFileSingleTokens, tokenizedFuncs)\n",
        "\n",
        "elif(USING_TOKEN_TYPE == 1):\n",
        "    if(os.path.isfile(pickleFilePairTokens)):\n",
        "        tokenizedFuncs = pickleOutFromFile(pickleFilePairTokens)\n",
        "    else:\n",
        "        tokenizedFuncs = [(tokenizeString(x[0]), x[1]) for x in functionCore]\n",
        "        pickleIntoFile(pickleFilePairTokens, tokenizedFuncs) \n",
        "\n",
        "elif(USING_TOKEN_TYPE == 2):\n",
        "    if(os.path.isfile(pickleFileClangTokens)):\n",
        "        tokenizedFuncs = pickleOutFromFile(pickleFileClangTokens)\n",
        "    else:\n",
        "        tokenizedFuncs = []\n",
        "        for x in functionCore:\n",
        "            open(\"tempWriteFile.c\", \"w\").write(x[0])\n",
        "            tokenizedFuncs.append((clangFileASTList(\"tempWriteFile.c\"), x[1]))\n",
        "        pickleIntoFile(pickleFileClangTokens, tokenizedFuncs)\n",
        "\n",
        "elif(USING_TOKEN_TYPE == 3):\n",
        "    if(os.path.isfile(pickleFileExtendedTokens)):\n",
        "        tokenizedFuncs = pickleOutFromFile(pickleFileExtendedTokens)\n",
        "    else:\n",
        "        tokenizedFuncs = [(tokenizeExtended(x[0]), x[1]) for x in functionCore]\n",
        "        pickleIntoFile(pickleFileExtendedTokens, tokenizedFuncs) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hpList = []\n",
        "\n",
        "if(os.path.isfile(pickleFileHPFiles)):\n",
        "    hpList = pickleOutFromFile(pickleFileHPFiles)\n",
        "else:\n",
        "    hpFile = h5py.File(\"VDISC_test.hdf5\", \"r\")\n",
        "    print(list(hpFile.keys()))\n",
        "\n",
        "    funcs = np.array(hpFile.get(\"functionSource\"))\n",
        "    funcs = [x.decode('utf-8') for x in funcs]\n",
        "    cwe119 = np.array(hpFile.get(\"CWE-119\"))  # BUFFER\n",
        "    cwe120 = np.array(hpFile.get(\"CWE-120\"))  # BUFFER\n",
        "    cwe469 = np.array(hpFile.get(\"CWE-469\"))  # MEMORY\n",
        "    cwe476 = np.array(hpFile.get(\"CWE-476\"))  # MEMORY\n",
        "    cweOther = np.array(hpFile.get(\"CWE-other\"))  # NUMERICAL (CHEAT)\n",
        "    if(USING_TOKEN_TYPE == 0):\n",
        "        for i, x in enumerate(funcs):\n",
        "            tokenized = tokenizeWithoutValue(x)\n",
        "            if(cwe119[i] or cwe120[i]):\n",
        "                hpList.append((tokenized, \"BUFFER\"))\n",
        "            elif(cwe469[i] or cwe476[i]):\n",
        "                hpList.append((tokenized, \"MEMORY\"))\n",
        "            elif(cweOther[i]):\n",
        "                # THIS IS A CHEAT FOR RIGHT NOW\n",
        "                hpList.append((tokenized, \"NUMERICAL\"))\n",
        "            else:\n",
        "                hpList.append((tokenized, \"CLEAN\"))\n",
        "\n",
        "    elif(USING_TOKEN_TYPE == 1):\n",
        "        hpList = [(tokenizeString(x), x[1]) for x in funcs]\n",
        "\n",
        "    elif(USING_TOKEN_TYPE == 2):\n",
        "        for i, x in enumerate(funcs):\n",
        "            f = open(\"tempWriteFile.c\", \"w\")\n",
        "            f.write(x[0])\n",
        "            tokenized = clangFileASTList(\"tempWriteFile.c\")\n",
        "            f.close()\n",
        "\n",
        "            if(cwe119[i] or cwe120[i]):\n",
        "                hpList.append((tokenized, \"BUFFER\"))\n",
        "            elif(cwe469[i] or cwe476[i]):\n",
        "                hpList.append((tokenized, \"MEMORY\"))\n",
        "            elif(cweOther[i]):\n",
        "                # THIS IS A CHEAT FOR RIGHT NOW\n",
        "                hpList.append((tokenized, \"NUMERICAL\"))\n",
        "            else:\n",
        "                hpList.append((tokenized, \"CLEAN\"))\n",
        "\n",
        "    elif(USING_TOKEN_TYPE == 3):\n",
        "        for i, x in enumerate(funcs):\n",
        "            tokenized = tokenizeExtended(x)\n",
        "            if(cwe119[i] or cwe120[i]):\n",
        "                hpList.append((tokenized, \"BUFFER\"))\n",
        "            elif(cwe469[i] or cwe476[i]):\n",
        "                hpList.append((tokenized, \"MEMORY\"))\n",
        "            elif(cweOther[i]):\n",
        "                # THIS IS A CHEAT FOR RIGHT NOW\n",
        "                hpList.append((tokenized, \"NUMERICAL\"))\n",
        "            else:\n",
        "                hpList.append((tokenized, \"CLEAN\"))\n",
        "                \n",
        "    pickleIntoFile(pickleFileHPFiles, hpList) "
      ],
      "metadata": {
        "id": "5FsXlParBWpH"
      },
      "id": "5FsXlParBWpH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "15f6067a",
      "metadata": {
        "id": "15f6067a"
      },
      "source": [
        "---\n",
        "\n",
        "# Machine Learning Setup Section\n",
        "- Turn (a list of a) list of tokens into ngrams of a given length\n",
        "- Output confusion matrix, accuracy and precision scores, and TP/FP/TN/FN results\n",
        "- Print ROC Curves for different classifiers\n",
        "- Prediction functions for the RFC\n",
        "- Prediction functions for the LSTM and CNN\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "646ed12b",
      "metadata": {
        "id": "646ed12b"
      },
      "outputs": [],
      "source": [
        "### GENERAL USE FUNCTIONS ###\n",
        "def create_ngrams(listOfTokens, n):\n",
        "    newList = []\n",
        "    for k in listOfTokens:\n",
        "        newSubList = []\n",
        "        for i in range(0, len(k)-n):\n",
        "            newSubList.append(tuple(k[i:i+n]))\n",
        "        newList.append(newSubList)\n",
        "    return newList\n",
        "\n",
        "def outputResults(yT, yP, fName, TF):\n",
        "    # Print Title (File Name)\n",
        "    if(fName != None):\n",
        "        half = \"-\" * ((64 - len(fName))//2)\n",
        "        print(half + fName + half)\n",
        "    \n",
        "    # Confusion Matrix Construction\n",
        "    C = confusion_matrix(yT, yP)\n",
        "    \n",
        "    # Labels\n",
        "    labs = [x for x in set(yT)]\n",
        "    labs.sort()\n",
        "    \n",
        "    # Figure Plot\n",
        "    fig, ax = plt.subplots()\n",
        "    ax = sns.heatmap(C, square=True, annot=True, cbar=False, \n",
        "                    fmt='g', cmap='viridis', norm=colors.LogNorm(),\n",
        "                    xticklabels=labs, yticklabels=labs)\n",
        "    plt.xlabel(\"Predicted Value\")\n",
        "    plt.ylabel(\"Actual Value\")\n",
        "    if(fName != None):\n",
        "        plt.savefig(f\"{fName}.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Print Out Classification Report\n",
        "    print(classification_report(yT, yP))\n",
        "    print(\"=\"*64)\n",
        "\n",
        "    # True / False Ratios (Only make sense for binary classifications)\n",
        "    if(TF and len(C) == 2):\n",
        "        tp = C[1][1]\n",
        "        fp = C[0][1]\n",
        "        tn = C[0][0]\n",
        "        fn = C[1][0]\n",
        "\n",
        "        print(\"=\"*32)\n",
        "        print(\"True Positive: \" + str(tp))\n",
        "        print(\"False Positive: \" + str(fp))\n",
        "        print(\"Ratio: \" + str( tp / (tp + fp) ))\n",
        "        print(\"=\"*32)\n",
        "        print(\"True Negative: \" + str(tn))\n",
        "        print(\"False Negative: \" + str(fn))\n",
        "        print(\"Ratio: \" + str( tn / (tn + fn) ))\n",
        "        print(\"=\"*32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "925de12d",
      "metadata": {
        "id": "925de12d"
      },
      "outputs": [],
      "source": [
        "### (TFIDF + RFC) RELATED CODE ####\n",
        "# ROC Curve (MIGHT MOVE LATER)\n",
        "def printRFCROCCurve(MODEL, TFIDF, xList, yList):\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    threshold = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    # Label/Classes\n",
        "    labs = [x for x in set(yList)]\n",
        "    labs.sort()\n",
        "\n",
        "    # Binarize Classes\n",
        "    yBinarized = label_binarize(yList, classes=labs)\n",
        "    \n",
        "    if(len(yBinarized[0]) == 1):\n",
        "        yTrue = [int(x) for x in yList]\n",
        "        if(len(CLASSDICT) != 2):\n",
        "            predCols = [max(x[0], max(x[2:])) for x in MODEL.predict_proba(getTransformedData(TFIDF, xList, NGRAMS))]\n",
        "        else:\n",
        "            predCols = [x[1] for x in MODEL.predict_proba(getTransformedData(TFIDF, xList, NGRAMS))]\n",
        "        \n",
        "        fpr, tpr, threshold = roc_curve(yTrue, predCols)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Plots\n",
        "        plt.plot(fpr, tpr, linestyle=\"--\", label=f\"GOOD vs BAD | AUC = {str(roc_auc)[:5]}\")\n",
        "    \n",
        "    else:      \n",
        "        for i in range(len(labs)):\n",
        "            binCols = [x[i] for x in yBinarized]\n",
        "            predCols = [x[i] for x in MODEL.predict_proba(getTransformedData(TFIDF, xList, NGRAMS))]\n",
        "\n",
        "            fpr[i], tpr[i], threshold[i] = roc_curve(binCols, predCols)\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "            # Plots\n",
        "            plt.plot(fpr[i], tpr[i], linestyle=\"--\", label=f\"{labs[i]} vs REST | AUC = {str(roc_auc[i])[:5]}\")\n",
        "\n",
        "    plt.plot([0,1], [0,1], \"--\", color=\"black\")\n",
        "    plt.xlim([0,1])\n",
        "    plt.ylim([0,1])\n",
        "\n",
        "    if(USING_TOKEN_TYPE == 0):\n",
        "        plt.title(\"ROC Curve for RFC - SIMPLE TOKENS\")\n",
        "    elif(USING_TOKEN_TYPE == 2):\n",
        "        plt.title(\"ROC Curve for RFC - CLANG TOKENS\")\n",
        "    elif(USING_TOKEN_TYPE == 3):\n",
        "        plt.title(\"ROC Curve for RFC - EXTENDED TOKENS\")\n",
        "\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def printCNNROCCurve(MODEL, xList, yList, pad, tokens):\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    threshold = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    # Label/Classes\n",
        "    labs = [x for x in set(yList)]\n",
        "    labs.sort()\n",
        "\n",
        "    # Binarize Classes\n",
        "    yBinarized = label_binarize(yList, classes=labs)\n",
        "\n",
        "    for i in range(len(labs)):\n",
        "        # To Save Data\n",
        "        predCols = [] \n",
        "        binCols = [x[i] for x in yBinarized]\n",
        "\n",
        "        # Create Data || ASSUME TENSORS\n",
        "        for u in xList:\n",
        "            # a = u.type(torch.FloatTensor)\n",
        "            # b = torch.unsqueeze(a, 0)\n",
        "            # f = torch.permute(b, (0, 2, 1))\n",
        "            \n",
        "            f = u.type(torch.FloatTensor)\n",
        "            newF = f.to(DEVICE)\n",
        "            d = MODEL(newF)[i]\n",
        "            predCols.append(d)\n",
        "\n",
        "            del newF\n",
        "        \n",
        "        # Generate Results\n",
        "        fpr[i], tpr[i], threshold[i] = roc_curve(binCols, predCols)\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        # Plots\n",
        "        plt.plot(fpr[i], tpr[i], linestyle=\"--\", label=f\"{labs[i]} vs REST | AUC = {str(roc_auc[i])[:5]}\")\n",
        "\n",
        "    plt.plot([0,1], [0,1], \"--\", color=\"black\")\n",
        "    plt.xlim([0,1])\n",
        "    plt.ylim([0,1])\n",
        "    if(USING_TOKEN_TYPE == 0):\n",
        "        plt.title(\"ROC Curve for CNN - SIMPLE TOKENS\")\n",
        "    elif(USING_TOKEN_TYPE == 2):\n",
        "        plt.title(\"ROC Curve for CNN - CLANG TOKENS\")\n",
        "    elif(USING_TOKEN_TYPE == 3):\n",
        "        plt.title(\"ROC Curve for CNN - EXTENDED TOKENS\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def predictCode(TFIDF, MODEL, CODE, FN, N_NGRAMS):\n",
        "    temp = create_ngrams([FN(CODE)], N_NGRAMS)\n",
        "    tokenMatrix = TFIDF.transform(temp)\n",
        "    return MODEL.predict(tokenMatrix)\n",
        "\n",
        "def getTransformedData(TFIDF, TOKENLIST, N_NGRAMS):\n",
        "    temp = create_ngrams(TOKENLIST, N_NGRAMS)\n",
        "    return TFIDF.transform(temp)\n",
        "    \n",
        "def predictTokens(TFIDF, MODEL, TOKENLIST, N_NGRAMS):\n",
        "    temp = create_ngrams(TOKENLIST, N_NGRAMS)\n",
        "    return MODEL.predict(TFIDF.transform(temp))\n",
        "\n",
        "def trainRFC(N_NGRAMS, N_ESTIMATORS, SPLITSIZE, LIST, TF, FNAME):\n",
        "    # Final Preprocessing\n",
        "    LOCALLIST = LIST\n",
        "    shuffle(LOCALLIST)\n",
        "\n",
        "    # Create Ngram Lists\n",
        "    tempList = [x[0] for x in LOCALLIST]\n",
        "    funcs = [(y, x[1]) for x, y in zip(LOCALLIST, create_ngrams(tempList, N_NGRAMS))]\n",
        "    \n",
        "    # Create X and Y lists for the data\n",
        "    funcsX = [x[0] for x in funcs]\n",
        "    funcsY = [x[1] for x in funcs]\n",
        "    \n",
        "    # Split into Train and Test Data\n",
        "    xTrain, xTest, yTrain, yTest = train_test_split(funcsX, funcsY, test_size=SPLITSIZE)\n",
        "    \n",
        "    # TFIDF Vectorization\n",
        "    tfidf = TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda y: y)\n",
        "    total = xTrain + xTest\n",
        "    tfFit = tfidf.fit_transform(total)\n",
        "    \n",
        "    # CREATE & TRAIN RFC\n",
        "    count = len(xTrain)\n",
        "    model = RandomForestClassifier(n_estimators=N_ESTIMATORS)\n",
        "    model.fit(tfFit[0:count], yTrain)\n",
        "    yPred = model.predict(tfFit[count:])\n",
        "    \n",
        "    # Output Results\n",
        "    outputResults(yTest, yPred, FNAME, TF)\n",
        "    \n",
        "    # Return Trained RFC Model\n",
        "    return tfidf, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e43ba508",
      "metadata": {
        "id": "e43ba508"
      },
      "outputs": [],
      "source": [
        "def createTensor(seq, pad):\n",
        "    minLen = min(pad, len(seq))\n",
        "\n",
        "    if(USING_TOKEN_TYPE == 0):\n",
        "        tensor = torch.zeros(pad, len(TOKENDICT))\n",
        "        for u in range(minLen):\n",
        "            index = 0\n",
        "            for i in TOKENDICT.keys():\n",
        "                if(seq[u] == i):\n",
        "                    index = TOKENDICT[i]\n",
        "                    break\n",
        "            tensor[u][index] = 1\n",
        "        for i in range(minLen, pad):\n",
        "            tensor[i] = torch.zeros(len(TOKENDICT))\n",
        "\n",
        "        #     index = [i for i in INVTOKENDICT if INVTOKENDICT[i] == seq[u]][0]\n",
        "        #     tensor[u][index] = 1\n",
        "        # for i in range(minLen, PADDING_LEN):\n",
        "        #         tensor[i] = torch.zeros(len(TOKENDICT))\n",
        "\n",
        "    elif(USING_TOKEN_TYPE == 3):\n",
        "        tensor = torch.zeros(pad, len(EXTENDEDTOKENS))\n",
        "        for u in range(minLen):\n",
        "            index = [i for i in INVEXTENDEDTOKENS if INVEXTENDEDTOKENS[i] == seq[u]][0]\n",
        "            tensor[u][index] = 1\n",
        "        for i in range(minLen, pad):\n",
        "            tensor[i] = torch.zeros(len(EXTENDEDTOKENS))\n",
        "    \n",
        "    elif(USING_TOKEN_TYPE == 2):\n",
        "        tensor = torch.zeros(pad, len(CLANGTOKENS))\n",
        "        for u in range(minLen):\n",
        "            index = 0\n",
        "            for i in CLANGTOKENS.keys():\n",
        "                if(seq[u] == i):\n",
        "                    index = CLANGTOKENS[i]\n",
        "                    break\n",
        "            tensor[u][index] = 1\n",
        "        for i in range(minLen, pad):\n",
        "            tensor[i] = torch.zeros(len(CLANGTOKENS))\n",
        "            \n",
        "    return tensor\n",
        "\n",
        "def getCategoryFromOutputTensor(out):\n",
        "    categoryIdx = torch.argmax(out).item()\n",
        "    return CLASSDICT[categoryIdx]\n",
        "\n",
        "def predict(MODEL, inputFunc, isLast):\n",
        "    with torch.no_grad():\n",
        "        out = MODEL(inputFunc)\n",
        "        if(not isLast):\n",
        "            return getCategoryFromOutputTensor(out)\n",
        "        else:\n",
        "            return getCategoryFromOutputTensor(out[-1])\n",
        "    \n",
        "def predictFunctions(MODEL, funcList, isLast):\n",
        "    retList = []\n",
        "    for k in funcList:\n",
        "        newK = k.to(DEVICE)\n",
        "        retList.append(predict(MODEL, newK, isLast))\n",
        "        del newK\n",
        "    return retList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5adc5f97",
      "metadata": {
        "id": "5adc5f97"
      },
      "outputs": [],
      "source": [
        "### RNN RELATED CODE ###\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, inSize, lstmLayers, hiddenSize1, hiddenSize2, numClasses, direction):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(inSize, hiddenSize1, num_layers=lstmLayers, bidirectional=direction)\n",
        "\n",
        "        self.lin1 = None\n",
        "        if(direction):\n",
        "            self.lin1 = nn.Linear(hiddenSize1*2, hiddenSize2)\n",
        "        else:\n",
        "            self.lin1 = nn.Linear(hiddenSize1, hiddenSize2)\n",
        "        self.lin2 = nn.Linear(hiddenSize2, numClasses)\n",
        "        self.drop = nn.Dropout(0.25)\n",
        "        self.soft = nn.Softmax(dim=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        outA, (h, c) = self.lstm(x)\n",
        "        outB = F.relu(self.drop(self.lin1(outA)))\n",
        "        outC = F.relu(self.drop(self.lin2(outB)))\n",
        "        # outD = self.soft(outC) ## NOT NEEDED WHEN USING nn.CrossEntropyLoss()\n",
        "        return outC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c333982",
      "metadata": {
        "id": "1c333982"
      },
      "outputs": [],
      "source": [
        "### CNN RELATED CODE ###\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, inSize, convSizes, linSizes, numClasses):\n",
        "        super(CNN, self).__init__()\n",
        "        self.kernels = [16, 8]\n",
        "        self.conv1 = nn.Conv1d(inSize, convSizes[0], self.kernels[0])\n",
        "        self.conv2 = nn.Conv1d(convSizes[0], convSizes[1], self.kernels[1])\n",
        "        self.lin1 = nn.Linear(convSizes[1] * (PADDING_LEN - sum(self.kernels) + len(self.kernels)), linSizes[0])\n",
        "        self.lin2 = nn.Linear(linSizes[0], numClasses)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outA = F.relu(self.conv1(x))\n",
        "        outB = F.relu(self.conv2(outA))\n",
        "        outB = torch.flatten(outB)\n",
        "        \n",
        "        outC = F.relu(self.lin1(outB))\n",
        "        outD = self.lin2(outC)\n",
        "        return outD"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b496d7bf",
      "metadata": {
        "id": "b496d7bf"
      },
      "source": [
        "---\n",
        "\n",
        "# Functionality Section\n",
        "### Final Before ML Stuff\n",
        "1. Separate tokenizedFuncs list into \"good\" and \"bad\" lists for balancing later\n",
        "2. Shuffle these lists to ensure \"fair\" chance at random functions being used\n",
        "3. Balance lists (same number of good and bad) into Training (both train and test) and Validation lists\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bdaf57c",
      "metadata": {
        "scrolled": false,
        "id": "6bdaf57c"
      },
      "outputs": [],
      "source": [
        "badJulietList = [x for x in tokenizedFuncs if x[1] != \"CLEAN\"]\n",
        "goodJulietList = [x for x in tokenizedFuncs if x[1] == \"CLEAN\"]\n",
        "\n",
        "badHPList = [x for x in hpList if x[1] != \"CLEAN\"]\n",
        "goodHPList = [x for x in hpList if x[1] == \"CLEAN\"]\n",
        "\n",
        "shuffle(goodHPList)\n",
        "shuffle(badHPList)\n",
        "shuffle(goodJulietList)\n",
        "shuffle(badJulietList)\n",
        "\n",
        "JNUM = int(0.33 * len(badJulietList))\n",
        "HNUM = int(0.5 * len(badHPList))\n",
        "\n",
        "RAT = 1.0\n",
        "\n",
        "JRAT = int(RAT * len(goodJulietList) / len(badJulietList))\n",
        "HRAT = int(RAT * len(goodHPList) / len(badHPList))\n",
        "\n",
        "badList = badJulietList[:JNUM] + badHPList[:HNUM]\n",
        "badValidateList = badJulietList[JNUM:] # + badHPList[HNUM:]\n",
        "\n",
        "goodList = goodJulietList[:JRAT * JNUM] + goodHPList[:HRAT * HNUM]\n",
        "goodValidateList = goodJulietList[JRAT * JNUM:] # + goodHPList[HRAT * HNUM: (HRAT*HNUM + len(goodJulietList))]\n",
        "\n",
        "### Binary Training Use\n",
        "if(USING_CLASSES == 1):\n",
        "    goodList = [(x[0], \"0\") for x in goodList]\n",
        "    badList = [(x[0], \"1\") for x in badList]\n",
        "\n",
        "    goodValidateList = [(x[0], \"0\") for x in goodValidateList]\n",
        "    badValidateList = [(x[0], \"1\") for x in badValidateList]\n",
        "\n",
        "trainList = badList + goodList\n",
        "validateList = badValidateList + goodValidateList\n",
        "validateListX = [x[0] for x in validateList]\n",
        "validateListY = [x[1] for x in validateList]\n",
        "\n",
        "shuffle(trainList)\n",
        "shuffle(validateList)\n",
        "\n",
        "print(\"=\"*40)\n",
        "print(f\"Good Label\\t| Buggy\\t| Train = {JNUM}\")\n",
        "print(f\"Good Label\\t| Clean\\t| Train = {HNUM}\")\n",
        "print(f\"Bad Label\\t| Buggy\\t| Train = {JNUM * JRAT}\")\n",
        "print(f\"Bad Label\\t| Clean\\t| Train = {HNUM * HRAT}\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Good Label\\t| Buggy\\t| Test = {len(badJulietList) - JNUM}\")\n",
        "print(f\"Good Label\\t| Clean\\t| Test = {len(goodJulietList) - (JNUM*JRAT)}\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Len of Train = {len(trainList)}\")\n",
        "print(f\"Len of Test = {len(validateList)}\")\n",
        "print(f\"Ratio Train/Test = {len(trainList) / len(validateList)}\")\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IGNORE THIS CODE\n",
        "# This was an attempt at using \"unknown\" files and testing them but it did not yield good results\n",
        "\n",
        "# unknownList = []\n",
        "\n",
        "# if(USING_TOKEN_TYPE == 0):\n",
        "#     unknownList = [(tokenizeWithoutValue(x[0]), x[1]) for x in unknownFiles]\n",
        "# elif(USING_TOKEN_TYPE == 1):\n",
        "#     unknownList = [(tokenizeString(x[0]), x[1]) for x in unknownFiles]\n",
        "# elif(USING_TOKEN_TYPE == 2):\n",
        "#     for x in unknownFiles:\n",
        "#         f = open(\"tempWriteFile.c\", \"w\")\n",
        "#         f.write(x[0])\n",
        "#         unknownList.append((clangFileASTList(\"tempWriteFile.c\"), x[1]))\n",
        "#         f.close()\n",
        "# elif(USING_TOKEN_TYPE == 3):\n",
        "#     unknownList = [(tokenizeExtended(x[0]), x[1]) for x in unknownFiles]"
      ],
      "metadata": {
        "id": "YE5RcO1j5VT2"
      },
      "id": "YE5RcO1j5VT2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def oneHotEncode(inputList, tokenList, encType, padLen):\n",
        "    ohe = []\n",
        "    \n",
        "    if(encType == 0):\n",
        "        ohe = [torch.zeros(padLen, NUM_TOKENS) for x in inputList]\n",
        "        for u in range(len(inputList)):\n",
        "            minLen = min(len(inputList[u][0]), padLen)\n",
        "            for v in range(minLen):\n",
        "                index = 0\n",
        "                for i in tokenList.keys():\n",
        "                    if(inputList[u][0][v] == i):\n",
        "                        index = tokenList[i]\n",
        "                        break\n",
        "                ohe[u][v][index] = 1\n",
        "            for i in range(minLen, padLen):\n",
        "                ohe[u][i] = torch.zeros(NUM_TOKENS)\n",
        "        # [functionX BY lenghtOfFuncX BY numOfTokens] where token type = 1\n",
        "\n",
        "    elif(encType == 1):\n",
        "        ohe = [torch.zeros(padLen) for x in inputList]\n",
        "        for u in range(len(inputList)):\n",
        "            minLen = min(len(inputList[u][0]), padLen)\n",
        "            for v in range(minLen):\n",
        "                index = 0\n",
        "                for i in tokenList.keys():\n",
        "                    if(inputList[u][0][v] == i):\n",
        "                        index = tokenList[i]\n",
        "                        break\n",
        "                ohe[u][v] = index\n",
        "            for i in range(minLen, padLen):\n",
        "                ohe[u][i] = -1\n",
        "        # [functionX BY lenghtOfFuncX] where VALUE = token type\n",
        "\n",
        "    return ohe"
      ],
      "metadata": {
        "id": "bvb50MeAFfWX"
      },
      "id": "bvb50MeAFfWX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RFC Running Code"
      ],
      "metadata": {
        "id": "IDeXe8T4YjRy"
      },
      "id": "IDeXe8T4YjRy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99393fa2",
      "metadata": {
        "scrolled": false,
        "id": "99393fa2"
      },
      "outputs": [],
      "source": [
        "TFIDF, RFC = trainRFC(NGRAMS, ESTIMATORS, TESTTRAINRATIO, trainList, True, f\"CWE-{NGRAMS}-{ESTIMATORS}-Training\")\n",
        "\n",
        "predicted = predictTokens(TFIDF, RFC, validateListX, NGRAMS)\n",
        "\n",
        "outputResults(validateListY, predicted, f\"CWE-{NGRAMS}-{ESTIMATORS}-Testing\", True)\n",
        "\n",
        "# outputResults([\"0\" if x == \"CLEAN\" else \"1\" for x in validateListY], [\"0\" if x == \"CLEAN\" else \"1\" for x in predicted], f\"CWE-{NGRAMS}-{ESTIMATORS}-Testing\", True)\n",
        "\n",
        "del predicted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "printRFCROCCurve(RFC, TFIDF, validateListX, validateListY)"
      ],
      "metadata": {
        "id": "MxKzFnXzcFIW"
      },
      "id": "MxKzFnXzcFIW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printRFCROCCurve(RFC, TFIDF, validateListX, [\"0\" if x == \"CLEAN\" else \"1\" for x in validateListY])"
      ],
      "metadata": {
        "id": "GpOWuLXkcG4r"
      },
      "id": "GpOWuLXkcG4r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "304014c7",
      "metadata": {
        "scrolled": false,
        "id": "304014c7"
      },
      "outputs": [],
      "source": [
        "# unknownPredict = predictTokens(TFIDF, RFC, [x[0] for x in unknownList], NGRAMS)\n",
        "\n",
        "# outputResults([x[1] for x in unknownList], unknownPredict, None, True)\n",
        "\n",
        "# outputResults([\"0\" if x[1] == \"CLEAN\" else \"1\" for x in unknownList], [\"0\" if x == \"CLEAN\" else \"1\" for x in unknownPredict], None, True)\n",
        "\n",
        "# del unknownPredict\n",
        "\n",
        "# probs = RFC.predict_proba(getTransformedData(TFIDF, [x[0] for x in unknownList], NGRAMS))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing for LSTM and CNN (NEED TO RUN)"
      ],
      "metadata": {
        "id": "o3Ks2v_3YoDL"
      },
      "id": "o3Ks2v_3YoDL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "865fb3da",
      "metadata": {
        "scrolled": true,
        "id": "865fb3da"
      },
      "outputs": [],
      "source": [
        "L = None\n",
        "\n",
        "# 0 -> Pad all token sequences to PADDING_LEN\n",
        "# 1 -> Do not pad the sequences (error when using CNN)\n",
        "ENCODING_TYPE = 0\n",
        "\n",
        "PADDING_LEN = -1\n",
        "\n",
        "if(USING_TOKEN_TYPE == 0):\n",
        "    L = TOKENDICT\n",
        "    PADDING_LEN = 1536\n",
        "elif(USING_TOKEN_TYPE == 2):\n",
        "    L = CLANGTOKENS\n",
        "    PADDING_LEN = 1536\n",
        "elif(USING_TOKEN_TYPE == 3):\n",
        "    L = EXTENDEDTOKENS\n",
        "    PADDING_LEN = 1536\n",
        "\n",
        "NUM_TOKENS = len(L)\n",
        "print(f\"#TOKENS = {NUM_TOKENS}\")\n",
        "\n",
        "# if(USING_TOKEN_TYPE == 0):\n",
        "#     oneHotEncodingFunctions = oneHotEncode(trainList, L, ENCODING_TYPE, PADDING_LEN)\n",
        "\n",
        "oneHotEncodingCategories = [torch.zeros(1, N_CLASSES) for _ in trainList]\n",
        "\n",
        "for u in range(len(oneHotEncodingCategories)):\n",
        "    index = [i for i in CLASSDICT if CLASSDICT[i] == trainList[u][1]][0]\n",
        "    oneHotEncodingCategories[u][0][index] = 1\n",
        "# [classificationX BY 1 BY numOfClasses] where classification type = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(CLASSDICT[1])"
      ],
      "metadata": {
        "id": "4dI6POi4w5xm"
      },
      "id": "4dI6POi4w5xm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9f51b49",
      "metadata": {
        "id": "a9f51b49"
      },
      "outputs": [],
      "source": [
        "maxLen = 0\n",
        "for x in validateListX:\n",
        "    if(len(x) > maxLen):\n",
        "        maxLen = len(x)\n",
        "print(maxLen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10a336a4",
      "metadata": {
        "id": "10a336a4"
      },
      "outputs": [],
      "source": [
        "print(oneHotEncodingCategories[0].size())\n",
        "# [#funcs BY 1 BY 5]\n",
        "\n",
        "# print(oneHotEncodingFunctions[0].size())\n",
        "# [#funcs BY padding BY NumTokens]\n",
        "# print(oneHotEncodingFunctions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Code"
      ],
      "metadata": {
        "id": "tI_qXDrfYsx1"
      },
      "id": "tI_qXDrfYsx1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c6d8998",
      "metadata": {
        "scrolled": false,
        "id": "4c6d8998"
      },
      "outputs": [],
      "source": [
        "N_LSTM_LAYERS = 3\n",
        "HIDDEN_DIM_1 = 128\n",
        "HIDDEN_DIM_2 = 64\n",
        "N_EPOCHS = 2\n",
        "LEARNING_RATE = 0.15\n",
        "if(USING_TOKEN_TYPE == 0):\n",
        "    LEARNING_RATE = 0.033\n",
        "if(USING_TOKEN_TYPE == 3):\n",
        "    LEARNING_RATE = 0.1\n",
        "BIDIRECTIONAL = False\n",
        "rnn = LSTM(len(L), N_LSTM_LAYERS, HIDDEN_DIM_1, HIDDEN_DIM_2, N_CLASSES, BIDIRECTIONAL).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=LEARNING_RATE, momentum=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3261d32e",
      "metadata": {
        "scrolled": false,
        "id": "3261d32e"
      },
      "outputs": [],
      "source": [
        "for E in range(N_EPOCHS):\n",
        "    avgLoss = 0\n",
        "    accScore = 0\n",
        "    mod = 7500\n",
        "\n",
        "    # Trying to reduce memory usage\n",
        "    oheF = [x[0] for x in trainList]\n",
        "\n",
        "    oheC = oneHotEncodingCategories\n",
        "\n",
        "    newList = list(zip(oheF, oheC))\n",
        "    shuffle(newList)\n",
        "    oheFN, oheCN = zip(*newList)\n",
        "\n",
        "    for i, (funcs, cats) in enumerate(zip(oheFN, oheCN)):\n",
        "        temp = funcs\n",
        "\n",
        "        # Trying to reduce memory\n",
        "        temp = torch.zeros(PADDING_LEN, NUM_TOKENS)\n",
        "        minLen = min(len(funcs), PADDING_LEN)\n",
        "        for v in range(minLen):\n",
        "            index = 0\n",
        "            for p in L.keys():\n",
        "                if(funcs[v] == p):\n",
        "                    index = L[p]\n",
        "                    break\n",
        "            temp[v][index] = 1\n",
        "        for o in range(minLen, PADDING_LEN):\n",
        "            temp[o] =  torch.zeros(NUM_TOKENS)\n",
        "\n",
        "        funcs = temp.type(torch.FloatTensor)\n",
        "        funcs = funcs.to(DEVICE)\n",
        "\n",
        "        cats = cats.type(torch.FloatTensor)\n",
        "        cats = cats.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outs = rnn(funcs)\n",
        "        loss = criterion(outs[-1], cats[0])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avgLoss += loss.item()\n",
        "        if torch.argmax(outs[-1]).item() == torch.argmax(cats[0]).item():\n",
        "            accScore += 1 \n",
        "        \n",
        "        if (i+1)%mod == 0:\n",
        "            print(f\"DONE {i+1} ITERATIONS\" + f\" || {str(100 * i / len(oheF))[:5]}% COMPLETE\" + f\" || AVG LOSS {avgLoss / i}\" + f\" || ACCURACY = {accScore / i}\")\n",
        "            print(\"=\"*96)\n",
        "            \n",
        "    print(f\"---- EPOCH {E+1} COMPLETE ----\\n\")\n",
        "    ###\n",
        "    # randFuncs = [x[0] for x in trainList]\n",
        "    # randCategories = [x[1] for x in trainList]\n",
        "    randFuncs = []\n",
        "    randCategories = []\n",
        "    for ri in range(len(validateListX)//5):\n",
        "        # ri = randint(0, len(validateListX)-1)\n",
        "        funcs = validateListX[ri]\n",
        "\n",
        "        temp = torch.zeros(PADDING_LEN, NUM_TOKENS)\n",
        "        minLen = min(len(funcs), PADDING_LEN)\n",
        "        for v in range(minLen):\n",
        "            index = 0\n",
        "            for p in L.keys():\n",
        "                if(funcs[v] == p):\n",
        "                    index = L[p]\n",
        "                    break\n",
        "            temp[v][index] = 1\n",
        "        for o in range(minLen, PADDING_LEN):\n",
        "            temp[o] =  torch.zeros(NUM_TOKENS)\n",
        "\n",
        "        funcs = temp.type(torch.FloatTensor)\n",
        "        randFuncs.append(funcs)\n",
        "        \n",
        "        randCategories.append(validateListY[ri])\n",
        "    predicateT = predictFunctions(rnn, randFuncs, True)\n",
        "    outputResults(randCategories, predicateT, None, True)\n",
        "\n",
        "    print(\"~\"*64)\n",
        "    # predV = [createTensor(u) for u in validateListX]\n",
        "    # predicateV = predictFunctions(rnn, predV, True)\n",
        "    # outputResults(validateListY, predicateV, None, True)\n",
        "    ###"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "randOuts = []\n",
        "predVals = []\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "threshold = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "# Label/Classes\n",
        "labs = [x for x in set(validateListY)]\n",
        "labs.sort()\n",
        "\n",
        "# Binarize Classes\n",
        "yBinarized = label_binarize(validateListY, classes=labs)\n",
        "ll = len(validateListX)\n",
        "for ri in range(ll):\n",
        "    newF = validateListX[ri]\n",
        "    temp = torch.zeros(PADDING_LEN, NUM_TOKENS)\n",
        "    minLen = min(len(newF), PADDING_LEN)\n",
        "    for v in range(minLen):\n",
        "        index = 0\n",
        "        for i in L.keys():\n",
        "            if(newF[v] == i):\n",
        "                index = L[i]\n",
        "                break\n",
        "        temp[v][index] = 1\n",
        "    for i in range(minLen, PADDING_LEN):\n",
        "        temp[i] = torch.zeros(NUM_TOKENS)\n",
        "\n",
        "    temp = temp.type(torch.FloatTensor)\n",
        "    # k = torch.unsqueeze(temp, 0)\n",
        "    # rf = torch.permute(k, (0, 2, 1))\n",
        "    rf = temp\n",
        "\n",
        "    with torch.no_grad():\n",
        "        newU = rf.to(DEVICE)\n",
        "        randOuts.append(rnn(newU).cpu()[-1])\n",
        "\n",
        "    del rf\n",
        "    del temp\n",
        "\n",
        "######\n",
        "\n",
        "# for i in range(len(labs)):\n",
        "#     # Generate Results\n",
        "#     fpr[i], tpr[i], threshold[i] = roc_curve([x[i] for x in yBinarized[:ll]], [e[i] for e in randOuts])\n",
        "#     roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "#     # Plots\n",
        "#     plt.plot(fpr[i], tpr[i], linestyle=\"--\", label=f\"{labs[i]} vs REST | AUC = {str(roc_auc[i])[:5]}\")\n",
        "#     print(f\"DONE {i+1} CLASSES\")\n",
        "\n",
        "######\n",
        "predCols = []\n",
        "\n",
        "for r in randOuts:\n",
        "    # x = max(r[0], max(r[2:])) # Use for multiclass\n",
        "    x = r[1]  # Use for Binary\n",
        "    predCols.append(x)\n",
        "        \n",
        "# fpr, tpr, threshold = roc_curve([0 if x == \"CLEAN\" else 1 for x in validateListY], predCols) # Use for multiclass\n",
        "fpr, tpr, threshold = roc_curve([0 if x == \"0\" else 1 for x in validateListY], predCols) # Use for binary\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "\n",
        "# Plots\n",
        "plt.plot(fpr, tpr, linestyle=\"--\", label=f\"CLEAN vs BUGGY | AUC = {str(roc_auc)[:5]}\")\n",
        "\n",
        "######\n",
        "\n",
        "\n",
        "###\n",
        "plt.plot([0,1], [0,1], \"--\", color=\"black\")\n",
        "plt.xlim([0,1])\n",
        "plt.ylim([0,1])\n",
        "if(USING_TOKEN_TYPE == 0):\n",
        "    plt.title(\"ROC Curve for LSTM - SIMPLE TOKENS\")\n",
        "elif(USING_TOKEN_TYPE == 2):\n",
        "    plt.title(\"ROC Curve for LSTM - CLANG TOKENS\")\n",
        "elif(USING_TOKEN_TYPE == 3):\n",
        "    plt.title(\"ROC Curve for LSTM - EXTENDED TOKENS\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.grid(True)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "###"
      ],
      "metadata": {
        "id": "KmhqWD777wXH"
      },
      "id": "KmhqWD777wXH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(validateListY[0], randOuts[0])\n",
        "outputResults(validateListY, [getCategoryFromOutputTensor(x) for x in randOuts], None, False)"
      ],
      "metadata": {
        "id": "KjKqgsRMeDxZ"
      },
      "id": "KjKqgsRMeDxZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN CODE"
      ],
      "metadata": {
        "id": "2K1ofCNmYwAB"
      },
      "id": "2K1ofCNmYwAB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "802aa8c0",
      "metadata": {
        "id": "802aa8c0"
      },
      "outputs": [],
      "source": [
        "# Using Original Token Types padded to 1536 (Very good for known-files / UNK for kernel files)\n",
        "# Using CLANG Token Types padded to 3072 ([SLOW] UNK for known-files / UNK for kernel files)\n",
        "\n",
        "N_EPOCHS = 2\n",
        "cnn = CNN(len(L), [256, 128], [64], N_CLASSES).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.33\n",
        "if(USING_TOKEN_TYPE == 2):\n",
        "    LEARNING_RATE = 0.01\n",
        "    MOMENTUM = 0.66\n",
        "if(USING_TOKEN_TYPE == 3):\n",
        "    LEARNING_RATE = 0.002\n",
        "    MOMENTUM = 0.25\n",
        "optimizer = torch.optim.SGD(cnn.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "240e5fac",
      "metadata": {
        "scrolled": false,
        "id": "240e5fac"
      },
      "outputs": [],
      "source": [
        "for E in range(N_EPOCHS):\n",
        "    avgLoss = 0\n",
        "    accScore = 0\n",
        "    mod = 7500\n",
        "\n",
        "    # Trying to reduce memory usage\n",
        "    oheF = [x[0] for x in trainList]\n",
        "\n",
        "    oheC = oneHotEncodingCategories\n",
        "\n",
        "    newList = list(zip(oheF, oheC))\n",
        "    shuffle(newList)\n",
        "    oheFN, oheCN = zip(*newList)\n",
        "\n",
        "    for i, (funcs, cats) in enumerate(zip(oheFN, oheCN)):\n",
        "        temp = funcs\n",
        "\n",
        "        # Trying to reduce memory\n",
        "        temp = torch.zeros(PADDING_LEN, NUM_TOKENS)\n",
        "        minLen = min(len(funcs), PADDING_LEN)\n",
        "        for v in range(minLen):\n",
        "            index = 0\n",
        "            for p in L.keys():\n",
        "                if(funcs[v] == p):\n",
        "                    index = L[p]\n",
        "                    break\n",
        "            temp[v][index] = 1\n",
        "        for o in range(minLen, PADDING_LEN):\n",
        "            temp[o] =  torch.zeros(NUM_TOKENS)\n",
        "\n",
        "        funcs = temp.type(torch.FloatTensor)\n",
        "\n",
        "        funcs = torch.unsqueeze(funcs, 0)\n",
        "        funcs = torch.permute(funcs, (0, 2, 1))\n",
        "        funcs = funcs.to(DEVICE)\n",
        "        \n",
        "        cats = cats.type(torch.FloatTensor)\n",
        "        cats = cats.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outs = cnn(funcs)\n",
        "        loss = criterion(outs, cats[0])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        del temp\n",
        "        del funcs\n",
        "\n",
        "        avgLoss += loss.item()\n",
        "        if torch.argmax(outs).item() == torch.argmax(cats[0]).item():\n",
        "            accScore += 1 \n",
        "\n",
        "        if (i+1)%mod == 0:\n",
        "            print(f\"DONE {i+1} ITERATIONS\" + f\" || {str(100 * i / len(oheF))[:5]}% COMPLETE\" + f\" || SET AVG LOSS {avgLoss / mod}\" + f\" || SET ACCURACY = {accScore / mod}\")\n",
        "            print(\"=\"*96)\n",
        "            accScore = 0\n",
        "            avgLoss = 0\n",
        "            \n",
        "    print(f\"---- EPOCH {E+1} COMPLETE ----\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "randOuts = []\n",
        "predVals = []\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "threshold = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "# Label/Classes\n",
        "labs = [x for x in set(validateListY)]\n",
        "labs.sort()\n",
        "\n",
        "# Binarize Classes\n",
        "yBinarized = label_binarize(validateListY, classes=labs)\n",
        "ll = len(validateListX)\n",
        "for ri in range(ll):\n",
        "    newF = validateListX[ri]\n",
        "    temp = torch.zeros(PADDING_LEN, NUM_TOKENS)\n",
        "    minLen = min(len(newF), PADDING_LEN)\n",
        "    for v in range(minLen):\n",
        "        index = 0\n",
        "        for i in L.keys():\n",
        "            if(newF[v] == i):\n",
        "                index = L[i]\n",
        "                break\n",
        "        temp[v][index] = 1\n",
        "    for i in range(minLen, PADDING_LEN):\n",
        "        temp[i] = torch.zeros(NUM_TOKENS)\n",
        "\n",
        "    temp = temp.type(torch.FloatTensor)\n",
        "    k = torch.unsqueeze(temp, 0)\n",
        "    rf = torch.permute(k, (0, 2, 1))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        newU = rf.to(DEVICE)\n",
        "        randOuts.append(cnn(newU).cpu())\n",
        "\n",
        "######\n",
        "\n",
        "# for i in range(len(labs)):\n",
        "#     # Generate Results\n",
        "#     fpr[i], tpr[i], threshold[i] = roc_curve([x[i] for x in yBinarized[:ll]], [e[i] for e in randOuts])\n",
        "#     roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "#     # Plots\n",
        "#     plt.plot(fpr[i], tpr[i], linestyle=\"--\", label=f\"{labs[i]} vs REST | AUC = {str(roc_auc[i])[:5]}\")\n",
        "#     print(f\"DONE {i+1} CLASSES\")\n",
        "\n",
        "######\n",
        "predCols = []\n",
        "\n",
        "for r in randOuts:\n",
        "    # x = max(r[0], max(r[2:])) # Use for multiclass\n",
        "    x = r[1] # Use for binary\n",
        "    predCols.append(x)\n",
        "        \n",
        "# fpr, tpr, threshold = roc_curve([0 if x == \"CLEAN\" else 1 for x in validateListY], predCols) # Use for multiclass\n",
        "fpr, tpr, threshold = roc_curve([0 if x == \"0\" else 1 for x in validateListY], predCols) # Use for binary\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "\n",
        "# Plots\n",
        "plt.plot(fpr, tpr, linestyle=\"--\", label=f\"CLEAN vs BUGGY | AUC = {str(roc_auc)[:5]}\")\n",
        "\n",
        "######\n",
        "\n",
        "\n",
        "###\n",
        "plt.plot([0,1], [0,1], \"--\", color=\"black\")\n",
        "plt.xlim([0,1])\n",
        "plt.ylim([0,1])\n",
        "if(USING_TOKEN_TYPE == 0):\n",
        "    plt.title(\"ROC Curve for CNN - SIMPLE TOKENS\")\n",
        "elif(USING_TOKEN_TYPE == 2):\n",
        "    plt.title(\"ROC Curve for CNN - CLANG TOKENS\")\n",
        "elif(USING_TOKEN_TYPE == 3):\n",
        "    plt.title(\"ROC Curve for CNN - EXTENDED TOKENS\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.grid(True)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "###"
      ],
      "metadata": {
        "id": "nYSrTVyxVVsm"
      },
      "id": "nYSrTVyxVVsm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(validateListY[0], randOuts[0])\n",
        "outputResults(validateListY, [getCategoryFromOutputTensor(x) for x in randOuts], None, False)"
      ],
      "metadata": {
        "id": "GhJ_32j9aUqH"
      },
      "id": "GhJ_32j9aUqH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unkFunc = []\n",
        "# unkCat = []\n",
        "# for i in range(len(unknownList)):\n",
        "#     newFunc = unknownList[i][0]\n",
        "#     newCat = unknownList[i][1]\n",
        "\n",
        "#     temp = torch.zeros(PADDING_LEN, NUM_TOKENS)\n",
        "#     minLen = min(len(newFunc), PADDING_LEN)\n",
        "\n",
        "#     for v in range(minLen):\n",
        "#         index = 0\n",
        "#         for i in L.keys():\n",
        "#             if(newFunc[v] == i):\n",
        "#                 index = L[i]\n",
        "#                 break\n",
        "#         temp[v][index] = 1\n",
        "#     for i in range(minLen, PADDING_LEN):\n",
        "#         temp[i] =  torch.zeros(NUM_TOKENS)\n",
        "\n",
        "#     temp = temp.type(torch.FloatTensor)\n",
        "#     k = torch.unsqueeze(temp, 0)\n",
        "#     unkFunc.append(torch.permute(k, (0, 2, 1)))\n",
        "#     unkCat.append(newCat)"
      ],
      "metadata": {
        "id": "ot8_QafvC5wu"
      },
      "id": "ot8_QafvC5wu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicateT = predictFunctions(cnn, unkFunc, False)\n",
        "# outputResults(unkCat, predicateT, None, True)\n",
        "\n",
        "# binaryPred = [\"0\" if k == \"CLEAN\" else \"1\" for k in predicateT]\n",
        "# binaryCats = [\"0\" if l == \"CLEAN\" else \"1\" for l in unkCat]\n",
        "\n",
        "# outputResults(binaryCats, binaryPred, None, True)"
      ],
      "metadata": {
        "id": "ssal2iA-jwlU"
      },
      "id": "ssal2iA-jwlU",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}